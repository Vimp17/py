import torch
from torch import nn
import torchvision
from torchvision import transforms
from torchsummary import summary
import torch.nn.functional as F
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

transform = transforms.Compose(
    [transforms.ToTensor()]
)

trainset = torchvision.datasets.MNIST(root='./data', train=True,download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)

testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                          shuffle=False, num_workers=2)

classes = tuple(str(i) for i in range(10))


class SimpleConvNet(nn.Module):
  def __init__(self):
    super(SimpleConvNet, self).__init__()

    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)
    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)
    self.fc1 = nn.Linear(4 * 4 * 16, 120)
    self.fc2 = nn.Linear(120, 84)
    self.fc3 = nn.Linear(84, 10)

  def forward(self, x):
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))

    #print(x.shape)

    x = x.view(-1, 4 * 4 * 16)
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.fc3(x)

    return x

  
from tqdm import tqdm_notebook
net  = SimpleConvNet().to(device)
loss_fn = torch.nn.CrossEntropyLoss()
learning_rate = 1e-4
optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
losses = []

## TRAINING model

fig = plt.figure(figsize=(10,7))

ax = fig.add_subplot(1,1,1)

for epoch in tqdm_notebook(range(2)):
  running_loss = 0.0
  for i,batch in enumerate(tqdm_notebook(trainloader)):
    X_batch, y_batch = batch

    optimizer.zero_grad()
    y_pred = net(X_batch.to(device))
    loss = loss_fn(y_pred, y_batch.to(device))

    loss.backward()
    optimizer.step()

    running_loss += loss.item()

    if i % 2000 == 1999:
      print('[%d, %5d] loss %.3f' %
            (epoch + 1, i + 1, running_loss/2000))
      losses.append(running_loss)
      running_loss = 0.0
    
  ax.clear()
  ax.plot(np.arange(len(losses)), losses)
  plt.show()

## Check on testloader

class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))

with torch.no_grad():
  for data in testloader:
      images, labels = data
      y_pred = net(images.to(device))
      _, predicted = torch.max(y_pred, 1)

      c = predicted.cpu().detach() == labels
      for i in range(4):
          label = labels[i]
          class_correct[label] +=c[i].item()
          class_total[label]+=1

for i in range(10):
      print('Accuracy of %5s : %2d %%' % (
          classes[i], 100 * class_correct[i]/class_total[i]
      ))



















  
