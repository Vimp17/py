import torch
import os
from __future__ import print_function, division
from skimage import io, transform

from torch import nn, optim
import torchvision
from torch.utils.data import Dataset, DataLoader

from torchvision import transforms, utils, datasets
from torchsummary import summary
import torch.nn.functional as F
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

class ModelBatchNorm(nn.Module):
  def __init__(self):
      super(ModelBatchNorm, self).__init__()
      self.conv1 = nn.Conv2d(3, 6, 3)
      self.bn1 = nn.BatchNorm2d(6)
      self.conv2 = nn.Conv2d(6, 16, 3)
      self.bn2 = nn.BatchNorm2d(16)
      self.fc1 = nn.Linear(16 * 6 * 6, 120)
      self.bn3 = nn.BatchNorm1d(120)
      self.fc2 = nn.Linear(120, 84)
      self.fc3 = nn.Linear(84, 10)

  def forward(self, x):
      
      x = self.bn1(F.max_pool2d(F.relu(self.conv1(x)), (2, 2)))
      x = self.bn2(F.max_pool2d(F.relu(self.conv2(x)), 2))

      x = x.view(x.shape[0], -1)
      x = self.bn3(F.relu(self.fc1(x)))
      x = F.relu(self.fc2(x))
      x = self.fc3(x)
      return x

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

def get_dataloaders(batch_size):
    transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    
    trainset = datasets.CIFAR10(root = './data', train=True, transform=transform, download=True)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

    testset = datasets.CIFAR10(root = './data', train=False, transform=transform, download=True)

    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

    return trainloader, testloader


def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    train_losses = []
    val_losses = []
    valid_accuraties = []

    for epoch in range(epochs):
        model.train()
        loss_sum = 0
        for xb, yb in tqdm(train_dl):
            xb, yb = xb.to(device), yb.to(device)

            opt.zero_grad()
            loss = loss_func(model(xb), yb)
            loss_sum += loss.item()

            loss.backward()
            opt.step()

        train_losses.append(loss_sum/len(train_dl))

        model.eval()
        loss_sum = 0
        correct = 0
        num = 0
        with torch.no_grad():
            correct = 0  # Счётчик правильных предсказаний
            num = 0  # Счётчик общего количества примеров
            loss_sum = 0  # Сумма потерь по всем батчам
            for xb, yb in tqdm(valid_dl):
              xb, yb = xb.to(device), yb.to(device)
              probs = model(xb)  # Выход модели
              loss_sum += loss_func(probs, yb).item()

              # Получаем предсказания и считаем правильные предсказания
              _, preds = torch.max(probs, axis=-1)
              correct += (preds == yb).sum().item()
              num += len(xb)  # Суммируем количество примеров

       # Сохраняем среднюю потерю и точность для всех батчей
        val_losses.append(loss_sum / len(valid_dl))  # Средняя потеря
        valid_accuraties.append(correct / num)

    return train_losses, val_losses, valid_accuraties



def plot_training(train_losses, valid_losses, valid_accuraties):
    plt.figure(figsize=(12,9))
    plt.subplot(2, 1, 1)
    plt.xlabel('epoch')
    plt.plot(train_losses, label='train loss')
    plt.plot(valid_losses, label='valid loss')
    plt.legend()
    plt.grid()

    plt.subplot(2, 1, 2)
    plt.xlabel('epoch')
    plt.plot(valid_accuraties, label='valid accuracy')
    plt.legend()
    plt.grid()


model = ModelBatchNorm().to(device)
crit = nn.CrossEntropyLoss()
opt = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)

inf = fit(10, model, crit, opt, *get_dataloaders(4))

plot_training(*inf)






