import torch
import os
from __future__ import print_function, division
from skimage import io, transform

from torch import nn, optim
import torchvision
from torch.utils.data import Dataset, DataLoader

from torchvision import transforms, utils, datasets
from torchsummary import summary
import torch.nn.functional as F
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

class Model(nn.Module):
  def __init__(self):
      super(Model, self).__init__()
      self.conv1 = nn.Conv2d(3, 6, 3)
      self.conv2 = nn.Conv2d(6, 16, 3)
      self.fc1 = nn.Linear(16 * 6 * 6, 120)
      self.fc2 = nn.Linear(120, 84)
      self.fc3 = nn.Linear(84, 10)

  def forward(self, x):
      
      x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
      x = F.max_pool2d(F.relu(self.conv2(x)), 2)

      x = x.view(x.shape[0], -1)
      x = F.relu(self.fc1(x))
      x = F.relu(self.fc2(x))
      x = self.fc3(x)
      return x

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

def get_dataloaders(batch_size):
    transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    
    trainset = datasets.CIFAR10(root = './data', train=True, transform=transform, download=True)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

    testset = datasets.CIFAR10(root = './data', train=False, transform=transform, download=True)

    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

    return trainloader, testloader



def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    train_losses = []
    val_losses = []
    valid_accuraties = []

    for epoch in range(epochs):
        model.train()
        loss_sum = 0
        for xb, yb in tqdm(train_dl):
            xb, yb = xb.to(device), yb.to(device)

            opt.zero_grad()
            loss = loss_func(model(xb), yb)
            loss_sum += loss.item()

            loss.backward()
            opt.step()

        train_losses.append(loss_sum/len(train_dl))

        model.eval()
        loss_sum = 0
        correct = 0
        num = 0
        with torch.no_grad():
                  for xb,yb in tqdm(valid_dl):
                      xb, yb = xb.to(device), yb.to(device)
                      probs = model(xb)

                      loss_sum = loss_func(probs,yb).item()

                      _, preds = torch.max(probs, axis=-1)
                      correct += (preds==yb).sum().item()
                      num+=len(xb)
        
        val_losses.append(loss_sum/len(valid_dl))
        valid_accuraties.append(correct/num)

    return train_losses, val_losses, valid_accuraties


def plot_training(train_losses, valid_losses, valid_accuraties):
    plt.figure(figsize=(12,9))
    plt.subplot(2, 1, 1)
    plt.xlabel('epoch')
    plt.plot(train_losses, label='train loss')
    plt.plot(valid_losses, label='valid loss')
    plt.legend()
    plt.grid()

    plt.subplot(2, 1, 2)
    plt.xlabel('epoch')
    plt.plot(valid_accuraties, label='valid accuracy')
    plt.legend()
    plt.grid()


model = Model().to(device)

crit = nn.CrossEntropyLoss()

opt = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)

inf = fit(10, model, crit, opt, *get_dataloaders(4))

plot_training(*inf)





